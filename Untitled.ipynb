{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "babi_train_raw, babi_test_raw = utils.get_babi_raw(\"1\", \"1\") # First argument is babi_id: babi task id, second is babi_id of test set\n",
    "\n",
    "word2vec = utils.load_glove(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### MODEL PARAMETERS ####\n",
    "\n",
    "TASK_1 = {\n",
    "    \"batch_size\": 1,\n",
    "    \"word_vector_length\": 50,\n",
    "    \"hidden_size\": 40,\n",
    "    \"attention_gate_hidden_size\": 40,\n",
    "    \"max_episodes\": 5,\n",
    "    \"max_input_sentences\": 10, # 40. The max input of task 1 is 10\n",
    "    \"max_epochs\": 15,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_classes\": 2,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"update_length\": 250\n",
    "}\n",
    "\n",
    "class DMNConfig(object):\n",
    "    \"\"\"\n",
    "    Default values\n",
    "    \"\"\"\n",
    "    def __init__(self, **params):\n",
    "        self.batch_size = params.get(\"batch_size\", 1)\n",
    "        self.word_vector_length = params.get(\"word_vector_length\", 50)\n",
    "        self.hidden_size = params.get(\"hidden_size\", 64)\n",
    "        self.attention_gate_hidden_size = params.get(\"attention_gate_hidden_size\", 64)\n",
    "        self.max_episodes = params.get(\"max_episodes\", 5)\n",
    "        self.max_input_sentences = params.get(\"max_input_sentences\", 10)\n",
    "        self.max_epochs = params.get(\"max_epochs\", 15)\n",
    "        self.learning_rate = params.get(\"learning_rate\", 0.001)\n",
    "#         self.learning_rate_gate = params.get(\"learning_rate_gate\", 1)\n",
    "        self.num_classes = params.get(\"num_classes\", 2)\n",
    "        self.dropout_prob = params.get(\"dropout_prob\", 0.1)\n",
    "        self.update_length = params.get(\"update_length\", 250)\n",
    "        # self.train_dataset = \n",
    "        # self.test_dataset = \n",
    "        # self.word_vectors =\n",
    "    \n",
    "\n",
    "class DMNModel():\n",
    "    \"\"\"\n",
    "    Most tasks in NLP can be cast in QA problems over language input.\n",
    "    This model (DMN) is a unified neural network framework which processes \n",
    "    input sequences and questions, forms semantic and episodic memories,\n",
    "    and generates relevant answers.\n",
    "    Questions trigger an iterative attention process with allow the model\n",
    "    to condition its attention on the result of previous iterations.\n",
    "    This results are then reasoned over in the hierarchical recurrent sequence\n",
    "    model to generate answers\n",
    "    \n",
    "    The model relies exclusively on trained word vector representations and \n",
    "    requires no string matching or manually engineered features.\n",
    "    \n",
    "    \n",
    "    TODO\n",
    "        Code organization:\n",
    "            - Organize as MemN2N from carpedm20 (github) and similar TF projects.\n",
    "                    - Separate data processing, config\n",
    "    \n",
    "        Data processing:\n",
    "            - Remove this from DMNClass\n",
    "            - Support Batches\n",
    "            - Shuffle data - Already supported in utils.py\n",
    "            - Check that embeddings is done efficiently (compare to a lookup)\n",
    "    \n",
    "        Improve graph:\n",
    "            - Implement changes in the episodic memory - avoid using gather!\n",
    "            - Calculate batch accuracy\n",
    "            - Implement DMN+ improvements\n",
    "                    - Regularization\n",
    "                    - Input Fusion layer\n",
    "                    - Untied weights\n",
    "            - Add methods for interactive prediction (give context, and question, return an answer)\n",
    "                    - Used later in the API\n",
    "            - Save and load results - tensorflow\n",
    "            - Gradient clipping  \n",
    "        \n",
    "        Visualization:\n",
    "        - TensorBoard\n",
    "        - API UI:   - Show episodes - probs., predict, feed examples, start learning, etc.\n",
    "                    - Different corpus - NER, POS, etc.\n",
    "                        - Slot filling - ATIS corpus\n",
    "                        - Sentiment analysis\n",
    "                        \n",
    "        New Features:\n",
    "                - VQA\n",
    "    \"\"\"\n",
    "    def __init__(self, params, babi_train_raw, babi_test_raw, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        \n",
    "        self.args = params\n",
    "        \n",
    "        self.vocab = {}\n",
    "        self.ivocab = {}\n",
    "        \n",
    "        self.input_mask_mode = \"sentence\" # Options: \"sentence\" or \"word\"\n",
    "        self.train_input, self.train_q, self.train_answer, self.train_gate, self.train_input_mask = self._process_input(babi_train_raw)\n",
    "        self.test_input, self.test_q, self.test_answer, self.test_gate, self.test_input_mask = self._process_input(babi_test_raw)\n",
    "\n",
    "        self.vocab_size = len(self.vocab)     \n",
    "        \n",
    "        self._build_model()\n",
    "    \n",
    "    def show_examples(self):\n",
    "        btrain1, btest1 = utils.get_babi_raw(\"1\", \"1\")\n",
    "        btrain2, btest2 = utils.get_babi_raw(\"2\", \"2\")\n",
    "        btrain3, btest3 = utils.get_babi_raw(\"3\", \"3\")\n",
    "        btrain4, btest4 = utils.get_babi_raw(\"4\", \"4\")\n",
    "        btrain5, btest5 = utils.get_babi_raw(\"5\", \"5\")\n",
    "        btrain6, btest6 = utils.get_babi_raw(\"6\", \"6\")\n",
    "        btrain7, btest7 = utils.get_babi_raw(\"7\", \"7\")\n",
    "        btrain8, btest8 = utils.get_babi_raw(\"8\", \"8\")\n",
    "        btrain9, btest9 = utils.get_babi_raw(\"9\", \"9\")\n",
    "        btrain10, btest10 = utils.get_babi_raw(\"10\", \"14\")\n",
    "        btrain11, btest11 = utils.get_babi_raw(\"11\", \"13\")\n",
    "        btrain12, btest12 = utils.get_babi_raw(\"12\", \"12\")\n",
    "        btrain13, btest13 = utils.get_babi_raw(\"13\", \"13\")\n",
    "        btrain14, btest14 = utils.get_babi_raw(\"14\", \"14\")\n",
    "        btrain15, btest15 = utils.get_babi_raw(\"15\", \"15\")\n",
    "        btrain16, btest16 = utils.get_babi_raw(\"16\", \"16\")\n",
    "        btrain17, btest17 = utils.get_babi_raw(\"17\", \"17\")\n",
    "        btrain18, btest18 = utils.get_babi_raw(\"18\", \"18\")\n",
    "        btrain19, btest19 = utils.get_babi_raw(\"19\", \"19\")\n",
    "        btrain20, btest20 = utils.get_babi_raw(\"20\", \"20\")\n",
    "    \n",
    "        print(\"Task 1\")\n",
    "        print(btrain1[0])\n",
    "        print(\"\\nTask 2\")\n",
    "        print(btrain2[0])\n",
    "        print(\"\\nTask 3\")\n",
    "        print(btrain3[0])\n",
    "        print(\"\\nTask 4\")\n",
    "        print(btrain4[0])\n",
    "        print(\"\\nTask 5\")\n",
    "        print(btrain5[0])\n",
    "        print(\"\\nTask 6\")\n",
    "        print(btrain6[0])\n",
    "        print(\"\\nTask 7\")\n",
    "        print(btrain7[0])\n",
    "        print(\"\\nTask 8\")\n",
    "        print(btrain8[0])\n",
    "        print(\"\\nTask 9\")\n",
    "        print(btrain9[0])\n",
    "        print(\"\\nTask 10\")\n",
    "        print(btrain10[0])\n",
    "        print(\"\\nTask 11\")\n",
    "        print(btrain11[0])\n",
    "        print(\"\\nTask 12\")\n",
    "        print(btrain12[0])\n",
    "        print(\"\\nTask 13\")\n",
    "        print(btrain13[0])\n",
    "        print(\"\\nTask 14\")\n",
    "        print(btrain14[0])\n",
    "        print(\"\\nTask 15\")\n",
    "        print(btrain15[0])\n",
    "        print(\"\\nTask 16\")\n",
    "        print(btrain16[0])\n",
    "        print(\"\\nTask 17\")\n",
    "        print(btrain17[0])\n",
    "        print(\"\\nTask 18\")\n",
    "        print(btrain18[0])\n",
    "        print(\"\\nTask 19\")\n",
    "        print(btrain19[0])\n",
    "        print(\"\\nTask 20\")\n",
    "        print(btrain20[0])\n",
    "\n",
    "    def _process_input(self, data_raw):\n",
    "        questions = []\n",
    "        inputs = []\n",
    "        answers = []\n",
    "        gate_values = []\n",
    "        input_masks = []\n",
    "        for x in data_raw:\n",
    "            inp = x[\"C\"].lower().split(' ')\n",
    "            inp = [w for w in inp if len(w) > 0]\n",
    "            q = x[\"Q\"].lower().split(' ')\n",
    "            q = [w for w in q if len(w) > 0]\n",
    "            \n",
    "            inp_vector = [utils.process_word(word = w,\n",
    "                                        word2vec = self.word2vec,\n",
    "                                        vocab = self.vocab,\n",
    "                                        ivocab = self.ivocab,\n",
    "                                        word_vector_size = self.args.word_vector_length,\n",
    "                                        to_return = \"word2vec\") for w in inp]\n",
    "\n",
    "            q_vector = [utils.process_word(word = w,\n",
    "                                        word2vec = self.word2vec,\n",
    "                                        vocab = self.vocab,\n",
    "                                        ivocab = self.ivocab,\n",
    "                                        word_vector_size = self.args.word_vector_length,\n",
    "                                        to_return = \"word2vec\") for w in q]            \n",
    "            \n",
    "            inputs.append(np.vstack(inp_vector).astype(np.float32))\n",
    "            questions.append(np.vstack(q_vector).astype(np.float32))\n",
    "            answers.append(utils.process_word(word = x[\"A\"],\n",
    "                                            word2vec = self.word2vec,\n",
    "                                            vocab = self.vocab,\n",
    "                                            ivocab = self.ivocab,\n",
    "                                            word_vector_size = self.args.word_vector_length,\n",
    "                                            to_return = \"index\"))\n",
    "            \n",
    "            gate_values.append(int(x[\"G\"]))\n",
    "            # NOTE: here we assume the answer is one word!\n",
    "            if self.input_mask_mode == 'word':\n",
    "                input_masks.append(np.array([index for index, w in enumerate(inp)], dtype=np.int32))\n",
    "            elif self.input_mask_mode == 'sentence':\n",
    "                input_masks.append(np.array([index for index, w in enumerate(inp) if w == '.'], dtype=np.int32))\n",
    "            else:\n",
    "                raise Exception(\"invalid input_mask_mode\")\n",
    "        \n",
    "        # TODO: Shuffle the training data\n",
    "        \n",
    "        return inputs, questions, answers, gate_values, input_masks\n",
    "\n",
    "    \n",
    "    def _add_placeholders(self):\n",
    "        \"\"\"\n",
    "        Generate placeholder variables to represent the input tensors\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        code and will be fed data during training.  Note that when \"None\" is in a\n",
    "        placeholder's shape, it's flexible.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 = Batch Size\n",
    "        self.input_placeholder = tf.placeholder(tf.float32, shape=[self.args.batch_size, None, self.args.word_vector_length])\n",
    "        self.input_length_placeholder = tf.placeholder(tf.int32, shape=[self.args.batch_size])\n",
    "        self.end_of_sentences_placeholder = tf.placeholder(tf.int32, shape=[self.args.batch_size, None])\n",
    "        self.question_placeholder = tf.placeholder(tf.float32, shape=[self.args.batch_size, None, self.args.word_vector_length])\n",
    "        self.question_length_placeholder = tf.placeholder(tf.int32, shape=[self.args.batch_size])\n",
    "        self.labels_placeholder = tf.placeholder(tf.float32, shape=[self.args.batch_size, self.vocab_size])\n",
    "        self.gate_placeholder = tf.placeholder(tf.float32, shape=[self.args.batch_size])\n",
    "\n",
    "\n",
    "    # Takes in an input matrix of size (number of words in input) x (WORD_VECTOR_LENGTH) with the input word vectors\n",
    "    # and a tensor the length of the number of sentences in the input with the index of the word that ends each\n",
    "    # sentence and returns a list of the states after the end of each sentence to be fed to other modules.\n",
    "    def _input_module(self):\n",
    "        \"\"\"\n",
    "        Computes a useful representation of the inputs such that the relevant\n",
    "        facts can be retrieved later.\n",
    "        \n",
    "        - Responsible for computing representations of (audio, visual or) textual \n",
    "        inputs, such that they can be retrieved when needed later.\n",
    "        - Assume a temporal sequence indexable by a time stamp.\n",
    "        - For written language we have a sequence of words (v1, v2, ..., vTw).\n",
    "        - Recurrent Neural Network (RNN) computation for context states -> GRU\n",
    "        \n",
    "        TODO:\n",
    "            - Make this more abstract for \"lego\" purposes\n",
    "            - Support visual input (DMN+). OOP-tensorflow?\n",
    "            - The new class should provide methods for visualization. \n",
    "                Word vectors, GRU results.\n",
    "        \"\"\"\n",
    "        # Compute the number of words in the input     \n",
    "        self.cell = tf.nn.rnn_cell.GRUCell(num_units=self.args.hidden_size, \n",
    "                                           input_size=self.args.word_vector_length)        \n",
    "        \n",
    "        # Get output after every word.\n",
    "        # outputs: (BATCH_SIZE, n_words, HIDDEN_SIZE)\n",
    "        # state: The last output (1, HIDDEN_SIZE)\n",
    "        outputs, state = tf.nn.dynamic_rnn(self.cell,\n",
    "                                           self.input_placeholder, \n",
    "                                           self.input_length_placeholder,\n",
    "                                           dtype='float32')\n",
    "      \n",
    "        context_rnn_out = tf.reshape(outputs, [-1, self.args.hidden_size]) # Now the shape = [n_words, n_units]\n",
    "\n",
    "        # Only project the state at the end of each sentence\n",
    "        sentence_representations_mat = tf.gather(context_rnn_out, \n",
    "                                                 self.end_of_sentences_placeholder) # (1, n_sentences, HIDDEN_SIZE)\n",
    "              \n",
    "        # The code below it necessary to split the Tensor(?,HIDDEN_SIZE) in a list of tensors with shape (1,HIDDEN_SIZE)\n",
    "\n",
    "        # Reshape `X` as a vector. -1 means \"set this dimension automatically\".\n",
    "        sentences_as_vector = tf.reshape(sentence_representations_mat, [-1])\n",
    "\n",
    "        # Create another vector containing zeroes to pad `X` to (MAX_INPUT_LENGTH * WORD_VECTOR_LENGTH) elements.\n",
    "        zero_padding = tf.zeros(\n",
    "            [self.args.max_input_sentences * self.args.hidden_size] - tf.shape(sentences_as_vector), \n",
    "            dtype=sentences_as_vector.dtype)\n",
    "\n",
    "        # Concatenate `X_as_vector` with the padding.\n",
    "        sentences_padded_as_vector = tf.concat(0, [sentences_as_vector, zero_padding])\n",
    "\n",
    "        # Reshape the padded vector to the desired shape.\n",
    "        sentences_padded = tf.reshape(sentences_padded_as_vector, [self.args.max_input_sentences,\n",
    "                                                                   self.args.hidden_size])\n",
    "\n",
    "        # Split X into a list of tensors of length MAX_INPUT_LENGTH where each tensor is a 1xHIDDEN_SIZE vector\n",
    "        # of the word vectors\n",
    "        self.sentence_representations = tf.split(0, self.args.max_input_sentences, sentences_padded)\n",
    "        \n",
    "        self.number_of_sentences = tf.shape(sentence_representations_mat)[1]\n",
    "        \n",
    "        self.input_only_for_testing = tf.concat(0, self.sentence_representations)\n",
    "\n",
    "        # Result of this module:\n",
    "        #                        - List of sentences. Each sentence is a tensor of shape (1, HIDDEN_UNITS). 1=Batch_size\n",
    "        #                        - The number of sentences.\n",
    "\n",
    "    \n",
    "    def _semantic_memory_module(self):\n",
    "        \"\"\"\n",
    "        Consists of:\n",
    "        1 - Stored word concepts\n",
    "        2 - Facts about them\n",
    "        \n",
    "        Initialized with GloVe vectos.\n",
    "        \n",
    "        This module could include gazeteers or other forms of explicit\n",
    "        knowledge bases.\n",
    "        \n",
    "        \n",
    "        Semantic Memory --> Episodic Memory\n",
    "        Semantic Memory <-> Input Text Sequences\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def _question_module(self):\n",
    "        \"\"\"\n",
    "        Simple GRU over question word vectors.\n",
    "        \n",
    "        qt = GRU(vt, qt-1)\n",
    "        \"\"\"\n",
    "        # Get output after every word.\n",
    "        # outputs: (1, n_words, HIDDEN_SIZE)\n",
    "        # state: The last output (1, HIDDEN_SIZE)\n",
    "        with tf.variable_scope(\"\", reuse=True):\n",
    "            outputs, state = tf.nn.dynamic_rnn(self.cell,\n",
    "                                               self.question_placeholder, \n",
    "                                               self.question_length_placeholder,\n",
    "                                               dtype='float32')\n",
    "        self.question_representation = state # BATCH_SIZE, HIDDEN_SIZE\n",
    "\n",
    "\n",
    "    def _episodic_memory_module(self):\n",
    "        \"\"\"\n",
    "        Retrieves facts from the input module conditioned on the question.\n",
    "        It then reasons over those facts to produce a final representation that\n",
    "        the answer module will use to generate an answer (memory).\n",
    "        Each pass produces an episode, an these episodes are then summarized into\n",
    "        the memory.\n",
    "        \n",
    "        - Combines the input and question modules in order to reason over them\n",
    "        and give the resulting knowledge to the answer module.\n",
    "        - Dynamically retrieves the necessary information over the sequence of\n",
    "        words or sentences.\n",
    "        - If necessary to retrieve additional facts -> iterate over inputs.\n",
    "        - Needed for transitive inference.\n",
    "        \n",
    "        \n",
    "        TODO:\n",
    "            - Refactor: - Remove (1). Using operations tile over the variables q and m.\n",
    "                                The resulting shape of z should be MAX_INPUT_SENTENCES, 1\n",
    "                                To get the tensor in for sentence t, do reshape [-1], and \n",
    "                                then use gather op. No need to do the gather in the input\n",
    "                                module.\n",
    "                                c : N_sent, H\n",
    "                                q : 1, H --tile--> N_sent, H\n",
    "                                z : N_sent, 1\n",
    "                                g : softmax(z) -> gt = softmax[t]\n",
    "                        - Extract from (2), the calculation for g. This should be a\n",
    "                                tf.nn.softmax(z)\n",
    "                        - Is it necessary to use the same weigths in the GRU? \n",
    "        \"\"\"\n",
    "        W_mem_res_in = tf.get_variable(\"W_mem_res_in\", shape=(self.args.hidden_size, \n",
    "                                                              self.args.hidden_size))\n",
    "        W_mem_res_hid = tf.get_variable(\"W_mem_res_hid\", shape=(self.args.hidden_size, \n",
    "                                                                self.args.hidden_size))\n",
    "        b_mem_res = tf.get_variable(\"b_mem_res\", shape=(self.args.hidden_size,))\n",
    "        \n",
    "        W_mem_upd_in = tf.get_variable(\"W_mem_upd_in\", shape=(self.args.hidden_size, \n",
    "                                                              self.args.hidden_size))\n",
    "        W_mem_upd_hid = tf.get_variable(\"W_mem_upd_hid\", shape=(self.args.hidden_size, \n",
    "                                                                self.args.hidden_size))\n",
    "        b_mem_upd = tf.get_variable(\"b_mem_upd\", shape=(self.args.hidden_size,))\n",
    "        \n",
    "        W_mem_hid_in = tf.get_variable(\"W_mem_hid_in\", shape=(self.args.hidden_size, \n",
    "                                                              self.args.hidden_size))\n",
    "        W_mem_hid_hid = tf.get_variable(\"W_mem_hid_hid\", shape=(self.args.hidden_size, \n",
    "                                                                self.args.hidden_size))\n",
    "        b_mem_hid = tf.get_variable(\"b_mem_hid\", shape=(self.args.hidden_size,))\n",
    "        \n",
    "        memory_states = [self.question_representation]\n",
    "        \n",
    "        with tf.variable_scope(\"memory\"):\n",
    "            gru_cell = tf.nn.rnn_cell.GRUCell(num_units = self.args.hidden_size)\n",
    "            for i in range(self.args.max_episodes):\n",
    "                z_vector = []\n",
    "                g_vector = []\n",
    "                e_vector = []\n",
    "                m_prev = memory_states[-1]\n",
    "                for t in range(self.args.max_input_sentences): # (1)\n",
    "                    s = self.sentence_representations[t]\n",
    "                    q = self.question_representation\n",
    "                    \n",
    "                    with tf.variable_scope(\"episode0\", reuse=True if (t > 0 or i > 0) else None):\n",
    "                        W_b = tf.get_variable(\"W_b\", shape=(self.args.hidden_size, self.args.hidden_size))\n",
    "                        \n",
    "                        W_1 = tf.get_variable(\"W_1\", shape=(4 * self.args.hidden_size, \n",
    "                                                            self.args.attention_gate_hidden_size))\n",
    "                        b_1 = tf.get_variable(\"b_1\", shape=(1, self.args.attention_gate_hidden_size))\n",
    "                        W_2 = tf.get_variable(\"W_2\", shape=(self.args.attention_gate_hidden_size, 1))\n",
    "                        b_2 = tf.get_variable(\"b_2\", shape=(1, 1))\n",
    "\n",
    "                        # Compute z\n",
    "                        # z(s, m, q) is BATCH_SIZE x (7 * HIDDEN_SIZE + 2)\n",
    "                        z_t = tf.concat(1, [tf.mul(s, q),\n",
    "                                            tf.mul(s, m_prev),\n",
    "                                            tf.abs(tf.sub(s, q)), # tf.pow(tf.sub(s, q), 2),\n",
    "                                            tf.abs(tf.sub(s, m_prev)) # tf.pow(tf.sub(s, m_prev), 2)\n",
    "                                           ])\n",
    "                        \n",
    "                        z_t = tf.add(tf.matmul(tf.tanh(tf.add(tf.matmul(z_t, W_1), b_1)), W_2), b_2)\n",
    "                        z_t = tf.exp(z_t)\n",
    "#                         print(\"z_t\")\n",
    "#                         print(z_t)\n",
    "                        self.z = tf.reduce_sum(z_t)\n",
    "                        z_vector.append(self.z) # This should be an scalar\n",
    "                \n",
    "\n",
    "                # Gating function as the attention mechanism\n",
    "                # g = G(s, m_prev, q)\n",
    "                # G returns a single scalar.\n",
    "                h_prev = tf.zeros((1, self.args.hidden_size))\n",
    "                for t in range(self.args.max_input_sentences): # (2)\n",
    "                    s = self.sentence_representations[t]\n",
    "                    g = z_vector[t] / tf.add_n(z_vector) # Softmax\n",
    "                    # TODO: \n",
    "                    #     - Add training capabilities - CE g\n",
    "                    #     - Add end of passes representation to the facts, and stop the \n",
    "                    #       iterative attention process if this representation is chosen by the \n",
    "                    #       gate function.\n",
    "                    self.g = tf.reduce_sum(g)\n",
    "                    g = self.g\n",
    "                    \n",
    "#                     print(\"g\")\n",
    "#                     print(g)\n",
    "                    g_vector.append(g) # This should be an scalar                    \n",
    "        \n",
    "                    # TODO: Figure out if it's necessary to share the weights?\n",
    "            \n",
    "                    # Compute the episode for pass i, using a modified GRU.\n",
    "                    r = tf.sigmoid(tf.add(tf.add(tf.matmul(s, W_mem_res_in),\n",
    "                                                 tf.matmul(h_prev, W_mem_res_hid)),\n",
    "                                          b_mem_res))\n",
    "                    _h = tf.nn.tanh(tf.add(tf.matmul(s, W_mem_hid_in),\n",
    "                                           tf.add(tf.mul(r, tf.matmul(h_prev, W_mem_hid_hid)), \n",
    "                                                  b_mem_hid)))\n",
    "                    h = tf.add(tf.mul(g, _h), tf.mul(tf.sub(1., g), h_prev))\n",
    "                    \n",
    "                    h_prev = h\n",
    "\n",
    "                    # TODO Replace the modified GRU using: gt * GRU(ct, hi-1) + (1-gt) * h_t-1\n",
    "                    # Look at other implementations\n",
    "                    \n",
    "                e = h_prev # e^i = h_tc -> Final state of the modified GRU.\n",
    "                # In case of a sequence modeling\n",
    "                # e^i = h_t -> for word t the episode is the state at t.\n",
    "                # And each word's unique m is sent independently to the answer module.\n",
    "                \n",
    "                with tf.variable_scope(\"\", reuse=True if i > 0 else None):\n",
    "                    # Summarize the episodes e^i into a memory, using the same GRU that\n",
    "                    # updates the attention mechanism's state.\n",
    "#                     _output, new_mem_state = gru_cell(e, m_prev)\n",
    "                    gru_h_prev = m_prev\n",
    "                    gru_x = e\n",
    "                    # TODO Figure out a way to use the same weigths using TF built in GRU\n",
    "                \n",
    "                    mem_u = tf.sigmoid(tf.add(tf.add(tf.matmul(gru_x, W_mem_upd_in),\n",
    "                                                     tf.matmul(gru_h_prev, W_mem_upd_hid)),\n",
    "                                              b_mem_upd))\n",
    "                    mem_r = tf.sigmoid(tf.add(tf.add(tf.matmul(gru_x, W_mem_res_in),\n",
    "                                                     tf.matmul(gru_h_prev, W_mem_res_hid)),\n",
    "                                              b_mem_res))\n",
    "                    mem_h_hat = tf.nn.tanh(tf.add(tf.matmul(gru_x, W_mem_hid_in),\n",
    "                                                  tf.add(tf.mul(mem_r, \n",
    "                                                                tf.matmul(gru_h_prev, W_mem_hid_hid)), \n",
    "                                                         b_mem_hid)))\n",
    "                    new_mem_state = tf.add(tf.mul(mem_u, mem_h_hat), tf.mul(tf.sub(1., mem_u), gru_h_prev))\n",
    "    \n",
    "                    memory_states.append(new_mem_state)\n",
    "            \n",
    "            # Return final memory state\n",
    "            self.episodic_memory_state = memory_states[-1]\n",
    "\n",
    "    \n",
    "    def _answer_module(self):\n",
    "        \"\"\"\n",
    "        This module decodes the memory into a sequence of words representing the\n",
    "        answer.\n",
    "        \n",
    "        - Simple GRU to produce an output at each of its time steps.\n",
    "        - Allow to predict end of sentence and stop.\n",
    "        \"\"\"\n",
    "        # a_0 = m\n",
    "        # a_t = GRU([y_t-1, q], a_t-1), y_t = softmax(W^a * a_t)\n",
    "        \n",
    "#         q = self.question_representation\n",
    "#         a_0 = self.episodic_memory_state\n",
    "#         y_t_1 = tf.nn.softmax(tf.matmul(a_0, W_a)) # 1 x vocab\n",
    "#         concat = tf.concat(1, [y_t_1, q])        \n",
    "        \n",
    "#         with tf.variable_scope(\"answer_module_gru\"):\n",
    "#             gru_cell_memory = tf.nn.rnn_cell.GRUCell(num_units=HIDDEN_SIZE)\n",
    "#             output, a = gru_cell_memory(concat, a_0)\n",
    "\n",
    "#         # Using GRU\n",
    "#         self.projections = tf.matmul(a, W_a)\n",
    "#         self.prediction = tf.nn.softmax(self.projections)\n",
    "        \n",
    "        # Without GRU\n",
    "#         self.projections = tf.matmul(a_0, W_out)\n",
    "#         self.prediction = tf.nn.softmax(self.projections)\n",
    "    \n",
    "        with tf.variable_scope(\"answer_module\"):\n",
    "            W_out = tf.get_variable(\"W_out\", shape=(self.args.hidden_size, self.vocab_size))\n",
    "            b_out = tf.get_variable(\"b_out\", shape=(1, self.vocab_size))\n",
    "\n",
    "        self.projections = tf.matmul(self.episodic_memory_state, W_out) + b_out\n",
    "        self.prediction = tf.nn.softmax(self.projections)\n",
    "\n",
    "\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        The semantic memory module (SMM) gives the words vectors (GloVe) to \n",
    "        the input module (IM), which computes the representation for each\n",
    "        sentence. The question module (QM) computes a representation of the \n",
    "        question too. The output of this two modules are used in the episodic\n",
    "        memory module (EMM) and reason over them. The output is then given to\n",
    "        the answer module (AM).\n",
    "        \n",
    "        For bAbI tasks the Mean Accuracy (%) should be 93.6.\n",
    "        \n",
    "        TODO:\n",
    "            - Cross entropy training of the gates.\n",
    "                bAbI posee en el dataset esta informacion, tratar de \n",
    "                generalizar la clase para soportar otros datasets\n",
    "                J = a * CE(Gates) + b * CE(Answers)\n",
    "        \"\"\"\n",
    "#         max_sent = 0\n",
    "#         for i in range(len(self.train_input)):\n",
    "#             if max_sent < len(self.train_input_mask[i]):\n",
    "#                 max_sent = len(self.train_input_mask[i])\n",
    "#                 print(max_sent)\n",
    "#         return\n",
    "        \n",
    "        self._add_placeholders()\n",
    "        self._input_module()        \n",
    "        self._question_module()        \n",
    "        self._episodic_memory_module()    \n",
    "        self._answer_module()\n",
    "        \n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "#         learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE, \n",
    "#                                                    global_step,          \n",
    "#                                                    1000, 0.1, staircase=True) # Every epoch\n",
    "\n",
    "        # Compute loss\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.projections, self.labels_placeholder))\n",
    "        \n",
    "        # Add optimizer\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost, global_step=global_step)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.args.learning_rate).minimize(cost)\n",
    "\n",
    "#         correct_prediction = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.labels_placeholder, 1))\n",
    "#         self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        # Initialize all variables\n",
    "        init = tf.initialize_all_variables()\n",
    "#         saver = tf.train.Saver()\n",
    "\n",
    "        print(\"Init finished!\")\n",
    "        \n",
    "        max_sent_length = 0\n",
    "        \n",
    "        # Train over multiple epochs\n",
    "        with tf.Session() as sess:\n",
    "            best_loss = float('inf')\n",
    "            best_val_epoch = 0\n",
    "            sess.run(init)\n",
    "            \n",
    "            # train until we reach the maximum number of epochs\n",
    "            for epoch in range(self.args.max_epochs):              \n",
    "                total_training_loss = 0\n",
    "                num_correct = 0\n",
    "                prev_prediction = 0\n",
    "                \n",
    "                print(\" \")\n",
    "                print('Epoch {}'.format(epoch))\n",
    "#                 start = time.time()\n",
    "\n",
    "                for i in range(len(self.train_input)): \n",
    "                    ans = np.zeros((1, self.vocab_size))\n",
    "                    ans[0][self.train_answer[i]] = 1\n",
    "\n",
    "                    # For debugging:\n",
    "                    # Input module: _input_tensor - self.input_only_for_testing\n",
    "                    # Question module: _question_representation - self.question_representation\n",
    "                    # Episode module: _e_i - self.e_i / _e_m_s - self.episodic_memory_state\n",
    "                    loss, _, pred_prob, _projections = sess.run(\n",
    "                        [cost, optimizer, self.prediction, self.projections],\n",
    "                        feed_dict={self.input_placeholder: [self.train_input[i]],\n",
    "                                   self.input_length_placeholder: [len(self.train_input[i])],\n",
    "                                   self.end_of_sentences_placeholder: [self.train_input_mask[i]],\n",
    "                                   self.question_placeholder: [self.train_q[i]],\n",
    "                                   self.question_length_placeholder: [len(self.train_q[i])],\n",
    "                                   self.labels_placeholder: ans,\n",
    "                                   self.gate_placeholder: [float(self.train_gate[i])]})\n",
    "        \n",
    "                    total_training_loss += loss\n",
    "        \n",
    "                    if np.argmax(pred_prob) == np.argmax(ans):\n",
    "                        num_correct += 1\n",
    "                    \n",
    "                    if i % self.args.update_length == 0:\n",
    "                        print \"Current average training loss: {}\".format(total_training_loss / (i + 1))\n",
    "                        print \"Current training accuracy: {}\".format(float(num_correct) / (i + 1))\n",
    "                        print(\"Ans: \" + str(self.ivocab[np.argmax(ans)]))\n",
    "                        print(\"Pred: \" + str(self.ivocab[np.argmax(pred_prob)]))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dmn_config = DMNConfig()\n",
    "dmn = DMNModel(dmn_config, babi_train_raw, babi_test_raw, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
